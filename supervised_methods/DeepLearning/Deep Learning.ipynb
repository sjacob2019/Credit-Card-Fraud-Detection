{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from deep_learning_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../../data/creditcard.csv'\n",
    "K_FOLDS = 10\n",
    "\n",
    "credit_card_df = pd.read_csv(DATA_PATH)\n",
    "credit_card_df = credit_card_df.sample(frac = 1).reset_index(drop = True) #shuffle the data by row\n",
    "\n",
    "# Split into dataset and labels\n",
    "dataset = credit_card_df.drop(columns=['Class']).to_numpy()\n",
    "labels = credit_card_df['Class'].to_numpy()\n",
    "\n",
    "# Normalize dataset via StandardScaler\n",
    "dataset = StandardScaler().fit_transform(dataset)\n",
    "\n",
    "histories = []\n",
    "results = []\n",
    "for train_index, test_index in KFold(n_splits=K_FOLDS).split(dataset):\n",
    "    x_train, y_train, x_test, y_test = get_splitted_data(dataset, labels, train_index, test_index)\n",
    "    model = get_compiled_model()\n",
    "    history = fit_model(model, x_train, y_train)\n",
    "    test_result = get_test_results(model, x_test, y_test)\n",
    "    histories.append(history)\n",
    "    results.append(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def display_metrics():\n",
    "specificities = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "balanced_accuracies = []\n",
    "loss = []\n",
    "\n",
    "test_specificities = []\n",
    "test_precisions = []\n",
    "test_recalls = []\n",
    "test_balanced_accuracies = []\n",
    "test_loss = []\n",
    "\n",
    "\n",
    "for i, history in enumerate(histories):\n",
    "    history = history.history\n",
    "    \n",
    "    loss.append(np.array(history['loss']))\n",
    "    test_loss.append(np.array(history['val_loss']))\n",
    "    \n",
    "    specificities.append(np.array(history['specificity']))\n",
    "    test_specificities.append(np.array(history['val_specificity']))\n",
    "    \n",
    "    precisions.append(np.array(history['precision']))\n",
    "    test_precisions.append(np.array(history['val_precision']))\n",
    "    \n",
    "    recalls.append(np.array(history['recall']))\n",
    "    test_recalls.append(np.array(history['val_recall']))\n",
    "    \n",
    "    balanced_accuracies.append((precisions[i] + recalls[i]) / 2)\n",
    "    test_balanced_accuracies.append((test_precisions[i] + test_recalls[i]) / 2)\n",
    "    \n",
    "    \n",
    "specificities = np.mean(np.array(specificities), axis=0)\n",
    "precisions = np.mean(np.array(precisions), axis=0)\n",
    "recalls = np.mean(np.array(recalls), axis=0)\n",
    "balanced_accuracies = np.mean(np.array(balanced_accuracies), axis=0)\n",
    "loss = np.mean(np.array(loss), axis=0)\n",
    "\n",
    "test_specificities = np.mean(np.array(test_specificities), axis=0)\n",
    "test_precisions = np.mean(np.array(test_precisions), axis=0)\n",
    "test_recalls = np.mean(np.array(test_recalls), axis=0)\n",
    "test_balanced_accuracies = np.mean(np.array(test_balanced_accuracies), axis=0)\n",
    "test_loss = np.mean(np.array(test_loss), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.arange(1, len(recalls) + 1)\n",
    "\n",
    "# Plotting Average Training Metrics\n",
    "plt.plot(epochs, balanced_accuracies, label='Balanced Accuracy')\n",
    "plt.plot(epochs, specificities, label='Specificity')\n",
    "plt.plot(epochs, precisions, label='Precision')\n",
    "plt.plot(epochs, recalls, label='Recall')\n",
    "plt.title(\"Average Training Metrics Per Epoch\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Metric Evaluation')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotting Average Validation Metrics\n",
    "plt.plot(epochs, test_balanced_accuracies, label='Balanced Accuracy')\n",
    "plt.plot(epochs, test_specificities, label='Specificity')\n",
    "plt.plot(epochs, test_precisions, label='Precision')\n",
    "plt.plot(epochs, test_recalls, label='Recall')\n",
    "plt.title(\"Average Validation Metrics Per Epoch\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Metric Evaluation')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotting Loss\n",
    "plt.plot(epochs, loss, label='Training Loss')\n",
    "plt.plot(epochs, test_loss, label='Validation Loss')\n",
    "plt.title(\"Average Training and Validation Loss\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_results = []\n",
    "for result in results:\n",
    "    avg_results.append(np.array(result[1:5] + [(result[4] + result[5]) / 2] + result[5:]))\n",
    "avg_results = np.mean(np.array(avg_results), axis=0)\n",
    "\n",
    "print(f'Specificity: {avg_results[1]}')\n",
    "print(f'Precision: {avg_results[2]}')\n",
    "print(f'Recall: {avg_results[3]}')\n",
    "print(f'Balanced Accuracy: {avg_results[4]}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "285a1a2a88ec4c06ab8af7baa253401f07b7487c84c964fda011e5216fa6ae9a"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('ml-project': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
