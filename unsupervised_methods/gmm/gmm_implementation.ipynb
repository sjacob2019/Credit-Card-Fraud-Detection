{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, confusion_matrix, ConfusionMatrixDisplay, fowlkes_mallows_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in CSV Data\n",
    "dataset = pd.read_csv('./data/creditcard.csv')\n",
    "len(dataset.columns.drop('Class'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utility functions for\n",
    "1. Splitting data into training and test\n",
    "2. Creating a Gaussian Mixture Model and Running the EM Algorithm\n",
    "3. Performing K-Fold Cross Validation with \n",
    "\"\"\"\n",
    "### Util functions to train models ###\n",
    "def get_train_test_splits(X: np.ndarray, Y: np.ndarray, train_index: np.ndarray, test_index: np.ndarray) -> tuple:\n",
    "    \"\"\"\n",
    "    Splits data and labels into train and test portions\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Normalized Data\n",
    "        Y (np.ndarray): Labels\n",
    "        train_index (np.ndarray): Indices corresponding to train data/labels from K-Fold\n",
    "        test_index (np.ndarray): Indices corresponding to test data/labels from K-Fold\n",
    "\n",
    "    Returns:\n",
    "        tuple: Returns x_train, x_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    x_train, x_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "def get_trained_model(x_train: np.ndarray, num_iterations: int) -> GaussianMixture:\n",
    "    \"\"\"\n",
    "    Creates a Gaussian Mixture Model and runs the EM Algorithm\n",
    "\n",
    "    Args:\n",
    "        x_train (np.ndarray): Training Data\n",
    "        num_iterations (int): Number of times to run the EM Algorithm\n",
    "\n",
    "    Returns:\n",
    "        GaussianMixture: Trained Gaussian Mixture Model\n",
    "    \"\"\"\n",
    "    gmm = GaussianMixture(n_components=2, n_init=num_iterations)\n",
    "    return gmm.fit(x_train)\n",
    "\n",
    "def get_model_predictions(model: GaussianMixture, x_test: np.ndarray, y_test: np.ndarray) -> tuple:\n",
    "    \"\"\"\n",
    "    Classifies test data based on trained Gaussian Model\n",
    "\n",
    "    Args:\n",
    "        model (GaussianMixture): The Trained Gaussian Mixture Model\n",
    "        x_test (np.ndarray): Testing data\n",
    "        y_test (np.ndarray): Testing Labels (the ground truth)\n",
    "\n",
    "    Returns:\n",
    "        tuple: x_test, y_test, predictions\n",
    "    \"\"\"\n",
    "    predictions = model.predict(x_test)\n",
    "    return x_test, y_test, predictions\n",
    "\n",
    "def train_and_test_model(unsupervised_data: np.ndarray) -> list:\n",
    "    \"\"\"\n",
    "    Runs K-Fold Validation on the entire data, training and getting predictions for each fold\n",
    "\n",
    "    Args:\n",
    "        unsupervised_data (np.ndarray): Entire data from preprocessing\n",
    "\n",
    "    Returns:\n",
    "        list: List of (x_test, y_test, predictions) for displaying results later\n",
    "    \"\"\"\n",
    "    K_FOLDS = 5\n",
    "    NUM_ITERATIONS = 10\n",
    "\n",
    "    results = []\n",
    "    count = 0\n",
    "    for train_index, test_index in KFold(shuffle=True, n_splits=K_FOLDS).split(unsupervised_data):\n",
    "        count += 1\n",
    "\n",
    "        # Splitting data and obtaining trained model\n",
    "        x_train, x_test, _, y_test = get_train_test_splits(unsupervised_data, labels, train_index, test_index)\n",
    "        \n",
    "        # Training Model\n",
    "        model = get_trained_model(x_train, NUM_ITERATIONS)\n",
    "\n",
    "        # Get Test Results for Models\n",
    "        test_result = get_model_predictions(model, x_test, y_test)\n",
    "        results.append(test_result)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Utility functions for \n",
    "1. Displaying Confusion Matrices\n",
    "2. Calculating recall_rates\n",
    "3. Calculating accuracies\n",
    "4. Calculating Silhouette Scores\n",
    "5. Calculating Fowlkes Mallows Scores\n",
    "6. Displaying clusterings with respect to V1 and V2\n",
    "'''\n",
    "### Util functions for displaying results\n",
    "def get_confusion_matrices(results: list) -> list:\n",
    "    \"\"\"\n",
    "    Generates confusion matrices given results\n",
    "\n",
    "    Args:\n",
    "        results (list): A list of (x_test, y_test, predictions) tuples\n",
    "\n",
    "    Returns:\n",
    "        list: A list of confusion matrices, returns a list because there is a\n",
    "              a confusion matrix for each for the folds in K-fold\n",
    "    \"\"\"\n",
    "    matrices_of_confusion = []\n",
    "    for i, (x_test, ground_truth, prediction) in enumerate(results):\n",
    "        cm = confusion_matrix(ground_truth, prediction)\n",
    "        if cm[0][0] < cm[0][1]:\n",
    "            # This means the clusters were flipped (0 means fraud 1 means fraud)\n",
    "            # so we switch our predictions\n",
    "            results[i] = (x_test, ground_truth, 1 - prediction)\n",
    "            cm = confusion_matrix(ground_truth, 1 - prediction)\n",
    "        matrices_of_confusion.append(cm)\n",
    "    return matrices_of_confusion\n",
    "\n",
    "def get_recall_rates(matrices_of_confusion: list) -> list:\n",
    "    \"\"\"\n",
    "    Calculates recall rates (percentage of frauds that were detected)\n",
    "\n",
    "    Args:\n",
    "        matrices_of_confusion (list): List of Confusion Matrices\n",
    "\n",
    "    Returns:\n",
    "        list: A list of recall rates, returns a list because there is a\n",
    "              a recall rate for each for the folds in K-fold\n",
    "    \"\"\"\n",
    "    recall_rates = []\n",
    "    for confusion_matrix in matrices_of_confusion:\n",
    "        recall = confusion_matrix[1][1] / (np.sum(confusion_matrix[1]))\n",
    "        recall_rates.append(recall)\n",
    "    return recall_rates\n",
    "\n",
    "def get_accuracies(results: list) -> list:\n",
    "    \"\"\"\n",
    "    Calculates accuracies (how well the model distinguishes fraud from not fraud)\n",
    "\n",
    "    Args:\n",
    "        results (list): A list of (x_test, y_test, predictions) tuples\n",
    "\n",
    "    Returns:\n",
    "        list: A list of accuracy rates, returns a list because there is a\n",
    "              a recall rate for each for the folds in K-fold\n",
    "    \"\"\"\n",
    "    accuracies = []\n",
    "    for _, y_test, prediction in results:\n",
    "        accuracy = np.count_nonzero(y_test == prediction) / len(y_test)\n",
    "        accuracies.append(accuracy)\n",
    "    return accuracies\n",
    "\n",
    "def get_silhouette_scores(results: list) -> list:\n",
    "    \"\"\"\n",
    "    Calculates silhouette_scores measured on a scale from -1 to 1\n",
    "    (how well the clusters are distinguished and how far apart they are)\n",
    "\n",
    "    Args:\n",
    "        results (list): A list of (x_test, y_test, predictions) tuples\n",
    "\n",
    "    Returns:\n",
    "        list: A list of silhouette scores, returns a list because there is a\n",
    "              a recall rate for each for the folds in K-fold\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for x_test, _, prediction in results:\n",
    "        score = silhouette_score(x_test, prediction)\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "def get_fowlkes_mallows_scores(results: list) -> list:\n",
    "    \"\"\"\n",
    "    Calculates Fowlkes Mallows Scores (similarities between 2 clusters)\n",
    "    https://scikit-learn.org/stable/modules/clustering.html#fowlkes-mallows-scores\n",
    "\n",
    "    Args:\n",
    "        results (list): A list of (x_test, y_test, predictions) tuples\n",
    "\n",
    "    Returns:\n",
    "        list: A list of Fowlkes Mallows Scores, returns a list because there is a\n",
    "              a recall rate for each for the folds in K-fold\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for _, ground_truth, prediction in results:\n",
    "        score = fowlkes_mallows_score(ground_truth, prediction)\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "def plot_clusterings(results: list) -> None:\n",
    "    \"\"\"\n",
    "    Plots Clustering with respect to V1 and V2 components\n",
    "\n",
    "    Args:\n",
    "        results (list): A list of (x_test, y_test, predictions) tuples\n",
    "    \"\"\"\n",
    "    for count, (x_test, y_test, prediction_test) in enumerate(results):\n",
    "        colors = {\n",
    "            0: \"green\",\n",
    "            1: \"red\"\n",
    "        }\n",
    "        for group in np.unique(y_test):\n",
    "            indexes = np.where(group == prediction_test)\n",
    "            plt.scatter(\n",
    "                x_test[indexes, 1],\n",
    "                x_test[indexes, 2],\n",
    "                c=colors[group],\n",
    "                s=5,\n",
    "                label=group,\n",
    "            )\n",
    "        plt.title(f\"Visualization of Clusterings using V1 and V2 for k={count + 1}\")    \n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main steps\n",
    "1. Shuffle the dataset\n",
    "2. Separate the labels from the data\n",
    "3. Normalize the data via MinMaxScalar\n",
    "4. Start from 2 columns and add components. For each component, run K-fold validation,\n",
    "   keeping track of time taken to run for each component added\n",
    "\"\"\"\n",
    "# Shuffling the dataset\n",
    "dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Separating the labels from the data\n",
    "labels = dataset['Class']\n",
    "unsupervised_data = dataset.drop(columns=['Class'])\n",
    "\n",
    "# Normalizing by Min/Max Scalar\n",
    "unsupervised_data = MinMaxScaler(feature_range=(0, 1)).fit_transform(unsupervised_data)\n",
    "\n",
    "times = []\n",
    "col_iteration_results = []\n",
    "for i in range(1, len(dataset.columns)):\n",
    "    desired_columns = dataset.columns[:i]\n",
    "    unsupervised_data = dataset[desired_columns]\n",
    "    unsupervised_data = MinMaxScaler(feature_range=(0, 1)).fit_transform(unsupervised_data)\n",
    "    start = datetime.now()\n",
    "    results = train_and_test_model(unsupervised_data)\n",
    "    end = datetime.now()\n",
    "    col_iteration_results.append(results)\n",
    "    times.append(math.ceil((end - start).total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying Confusion Matrices for the iteration with all components\n",
    "matrices_of_confusion = [get_confusion_matrices(result) for result in col_iteration_results]\n",
    "for matrix in matrices_of_confusion[-1]:\n",
    "    ConfusionMatrixDisplay(matrix).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display other metrics\n",
    "accuracies, recalls, silhouette_scores, fowlkes_mallows_scores = [], [], [], []\n",
    "\n",
    "for results in col_iteration_results:\n",
    "    accuracies.append(np.mean(get_accuracies(results)))\n",
    "    recalls.append(np.mean(get_recall_rates(get_confusion_matrices(results))))\n",
    "    silhouette_scores.append(get_silhouette_scores([results[0]]))\n",
    "    fowlkes_mallows_scores.append(np.mean(get_fowlkes_mallows_scores(results)))\n",
    "num_components = np.arange(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Number of Components vs Time Taken to run GMM\n",
    "plt.plot(num_components, times / 60)\n",
    "plt.title(\"Number of Components vs Time Taken\")\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.ylabel(\"Time Taken (min)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(num_components, accuracies * 100)\n",
    "plt.title(\"Number of Components used vs. Accuracy\")\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.ylabel(\"Percent Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(num_components, recall_rates * 100)\n",
    "plt.title(\"Number of Components Used vs. Recall Rates\")\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.ylabel(\"Percent Recall\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(num_components, silhouette_scores)\n",
    "plt.title(\"Number of Components used vs. Silhouette Score\")\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(num_components, fowlkes_mallows_scores)\n",
    "plt.title(\"Number of Components used Vs. Fowlkes Mallows Scores\")\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.ylabel(\"Fowlkes Mallows Score\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "285a1a2a88ec4c06ab8af7baa253401f07b7487c84c964fda011e5216fa6ae9a"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('ml-project': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
